spec: &spec
  x-sub-request-filters:
    - type: default
      name: http
      options:
        allow:
          - pattern: /^https?:\/\//
            forward_headers:
              user-agent: true
  title: The Change Propagation for JobQueue root
  paths:
    /{api:sys}/dedupe:
      x-modules:
        - path: src/sys/deduplicator.js
          options:
            redis_prefix: 'CPJQ'
            redis:
              host: <%= @redis_host %>
              port: 6379
              password: '<%= @redis_password %>'
    /{api:sys}/queue:
      x-modules:
        - path: src/sys/kafka.js
          options:
            metadata_broker_list: <%= @broker_list %>
            dc_name: default
            consumer:
              fetch.message.max.bytes: 4194304
              log.connection.close: false
            producer:
              compression.codec: snappy
              log.connection.close: false
            concurrency: 10
            startup_delay: 60000
            disable_blacklist: true
            disable_ratelimit: true
            templates:
<%- @high_traffic_jobs_config.each do |topic, topic_config| %>
              <%= topic %>:
                topic: 'mediawiki.job.<%= topic %>'
<%- topic_config.each do |key, value| %>
                <%= key %>: <%= value %>
<%- end %>
                exec:
                  method: post
                  uri: '<%= @jobrunner_host %>/rpc/RunSingleJob.php'
                  headers:
                    content-type: 'application/json'
                  body: '{{globals.message}}'
                  agentOptions:
                    keepAlive: true
<%- end %>
# Now special rule to cover all the low-traffic jobs
              low_traffic_jobs:
                concurrency: <%= @low_traffic_concurrency %>
                topics:
                  - '/^mediawiki\.job\..*/'
                # Don't execute anything that's covered by different rules
                exclude_topics:
<%- @high_traffic_jobs_config.keys.each do |job| %>
                  - 'mediawiki.job.<%= job %>'
<%- end %>
                  - 'mediawiki.job.ThumbnailRender'
                exec:
                  method: post
                  uri: '<%= @jobrunner_host %>/rpc/RunSingleJob.php'
                  headers:
                    content-type: 'application/json'
                  body: '{{globals.message}}'
                  agentOptions:
                    keepAlive: true

# Number of worker processes to spawn.
# Set to 0 to run everything in a single process without clustering.
# Use 'ncpu' to run as many workers as there are CPU units
num_workers: ncpu

# Number of workers to start in parallel after the first worker.
# The first worker is always started independently. After it has completed
# its start-up, this number controls the number of workers to start in
# parallel until `num_workers` have been started. Note that setting this
# number to a too high a value might lead to high resource consumption
# (especially of CPU) during the start-up process.
startup_concurrency: 4

# Log error messages and gracefully restart a worker if v8 reports that it
# uses more heap (note: not RSS) than this many mb.
worker_heap_limit_mb: 750

# The maximum interval in ms that can pass between two beat messages
# sent by each worker to the master before it is killed
worker_heartbeat_timeout: 15000

# Logger info
logging:
  level: warn
  name: cpjobqueue
  streams:
    - level: info
      path: /tmp/cpjq.log
      type: file
    - type: syslog
      host: localhost
      port: 10514
      prefix: '@cee: '
      name: node
  sampled_levels:
    trace/dedupe: 0.001
    trace/sample: 0.00002

# Statsd metrics reporter
metrics:
  name: cpjobqueue
  host: 10.0.15.145
  port: 8125
  type: statsd

services:
  - name: cpjobqueue
    module: hyperswitch
    conf:
      cors: "*"
      port: 7200
      proxy: '<%= @proxy %>'
      user_agent: ChangePropagation-JobQueue/WT
      spec: *spec
